name: CI

on:
  push:
    branches:
      - main
  pull_request:

env:
  DEFAULT_PYTHON: "3.12"
  DEFAULT_OS: ubuntu-latest

jobs:
  pytest:
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        python-version: ["3.12", "3.13"]
        # TODO: skip spark on Windows
        #os: [ubuntu-latest, windows-latest]
        os: [ubuntu-latest]

    steps:
    - uses: actions/checkout@v4
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        python -m pip install ".[dev]" --group=pyhive
        wget https://dlcdn.apache.org/spark/spark-4.0.1/spark-4.0.1-bin-hadoop3.tgz
        tar -xzf spark-4.0.1-bin-hadoop3.tgz
        export SPARK_HOME=$(pwd)/spark-4.0.1-bin-hadoop3
        export PATH=$SPARK_HOME/sbin:$PATH
        start-thriftserver.sh
    - name: Run pytest with coverage
      run: |
        # TODO: not sure why this isn't working now
        # CHRONIFY_HIVE_URL=hive://localhost:10000/default pytest -v --cov --cov-report=xml
        pytest -v --cov --cov-report=xml
    - name: codecov
      uses: codecov/codecov-action@v4.2.0
      if: ${{ matrix.os == env.DEFAULT_OS && matrix.python-version == env.DEFAULT_PYTHON  }}
      with:
        token: ${{ secrets.CODECOV_TOKEN }}
        name: chronify-tests
        fail_ci_if_error: false
        verbose: true
  mypy:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: 3.12
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        python -m pip install ".[dev]"
        mypy
  ruff:
    runs-on: ubuntu-latest
    name: "ruff"
    steps:
      - uses: actions/checkout@v4
      - uses: chartboost/ruff-action@v1
        with:
          src: "./src"
